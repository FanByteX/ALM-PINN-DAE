#!/bin/bash
# ========================================================================
# MindSpore 4å¡ NPU åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
# ä½¿ç”¨ MindSpore çš„ mpirun è¿›è¡Œå¤šå¡å¹¶è¡Œè®­ç»ƒ
# ========================================================================
set -e
echo "========================================================================"
echo "ğŸš€ Starting Distributed Training on 4 Ascend NPUs"
echo "========================================================================"
# ä½¿ç”¨ä¸å•å¡æœ€ä½³å‚æ•°ä¸€è‡´çš„é…ç½®ï¼Œé’ˆå¯¹V3æŒ¯è¡å¢å¼ºä»£æ•°çº¦æŸ
EPOCHS=20000
BATCH_SIZE=1048  # æ¯å¡batch sizeï¼Œä¸å•å¡ä¸€è‡´
LR=1e-5  # ä¸å•å¡æœ€ä½³å‚æ•°ç›¸åŒ
LOG_DIR="logs/mindspore_pinn_4npu"
# æƒé‡é…ç½®ï¼šDYN_WEIGHTä¿æŒå•å¡æœ€ä½³ï¼ŒALG_WEIGHTå¢å¤§è§£å†³V3æŒ¯è¡
DYN_WEIGHT=64.0   # ä¸å•å¡æœ€ä½³ä¸€è‡´
ALG_WEIGHT=10.5   # ä»1.0å¢å¤§åˆ°10.0ï¼Œä¸“é—¨å¼ºåŒ–V3ï¼ˆä»£æ•°å˜é‡ï¼‰çº¦æŸ
# ä»å½“å‰4å¡è®­ç»ƒçš„æœ€ä½³æ¨¡å‹ç»§ç»­ä¼˜åŒ–ï¼ˆä¸“æ³¨V3ï¼‰
PRETRAIN_MODEL="${LOG_DIR}/model.ckpt"
START_FROM_BEST=""

if [ -f "${PRETRAIN_MODEL}" ]; then
    echo "âœ… Found 4-card trained model: ${PRETRAIN_MODEL}"
    echo "ğŸ¯ Will continue training to improve V3 fitting"
    START_FROM_BEST="--start-from-best"
    export PRETRAIN_MODEL_PATH="${PRETRAIN_MODEL}"
else
    echo "âš ï¸  4-card model not found, trying single-card model..."
    PRETRAIN_MODEL="logs/mindspore_pinn_single_npu0/model.ckpt"
    if [ -f "${PRETRAIN_MODEL}" ]; then
        echo "âœ… Found single-card trained model: ${PRETRAIN_MODEL}"
        START_FROM_BEST="--start-from-best"
        export PRETRAIN_MODEL_PATH="${PRETRAIN_MODEL}"
    else
        echo "âš ï¸  No pretrained model found, starting from scratch"
    fi
fi
# æ¸…ç†æ—§çš„æ—¥å¿—
mkdir -p ${LOG_DIR}
# ä½¿ç”¨ mpirun å¯åŠ¨4å¡åˆ†å¸ƒå¼è®­ç»ƒ
echo ""
echo "ğŸ“Š Training Configuration:"
echo "  - NPU Cards: 4"
echo "  - Per-Card Batch Size: ${BATCH_SIZE}"
echo "  - Total Throughput: $((BATCH_SIZE * 4)) samples/iteration"
echo "  - Learning Rate: ${LR}"
echo "  - Epochs: ${EPOCHS}"
echo "  - Dynamic Weight: ${DYN_WEIGHT}"
echo "  - Algebraic Weight: ${ALG_WEIGHT}"
echo "  - Network: Attention-based (unstacked)"
echo "  - Strategy: Same as single-card best parameters"
echo "  - Log Directory: ${LOG_DIR}"
echo ""
# MindSpore åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å‘½ä»¤
# ä½¿ç”¨ mpirun åœ¨4ä¸ªNPUä¸Šå¯åŠ¨è®­ç»ƒ
mpirun -n 4 \
    --output-filename ${LOG_DIR}/rank_log \
    --allow-run-as-root \
    python mindspore_pinn.py \
    --distributed \
    --log-dir ${LOG_DIR} \
    --epochs ${EPOCHS} \
    --batch-size ${BATCH_SIZE} \
    --lr ${LR} \
    --num-train 6000 \
    --num-val 100 \
    --num-test 500 \
    --use-scheduler \
    --scheduler-type plateau \
    --patience 2000 \
    --unstacked \
    --dyn-type attention \
    --alg-type attention \
    --dyn-activation sin \
    --alg-activation sin \
    --dyn-depth 4 \
    --dyn-width 100 \
    --dyn-weight ${DYN_WEIGHT} \
    --alg-weight ${ALG_WEIGHT} \
    --h 0.1 \
    --N 160 \
    --test-every 1000 \
    --use-tqdm \
    ${START_FROM_BEST}

echo ""
echo "========================================================================"
echo "âœ… Distributed Training Completed!"
echo "========================================================================"
echo "ğŸ“ Results saved to: ${LOG_DIR}/"
echo "ğŸ“Š Check loss curve: ${LOG_DIR}/loss.png"
echo "ğŸ“ˆ Check trajectories: ${LOG_DIR}/trajectories.png"
echo "========================================================================"

#   è®­ç»ƒç»“æœï¼šBest at step 20000:
#   Train Loss: 1.238e-05
#   Test Loss: 1.233e-05
#   Test Metrics: []
# 'train' took 6682.037024 s
